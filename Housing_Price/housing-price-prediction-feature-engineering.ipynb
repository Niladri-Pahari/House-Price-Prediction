{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Load Necessary Libraries","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\n\n# Mute warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:05.792285Z","iopub.execute_input":"2022-08-13T20:29:05.792602Z","iopub.status.idle":"2022-08-13T20:29:05.799661Z","shell.execute_reply.started":"2022-08-13T20:29:05.792578Z","shell.execute_reply":"2022-08-13T20:29:05.798178Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Function to Clean Data","metadata":{}},{"cell_type":"code","source":"def clean(df):\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n    # Some values of GarageYrBlt are corrupt, so we'll replace them\n    # with the year the house was built\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Names beginning with numbers are awkward to work with\n    df.rename(columns={\n        \"1stFlrSF\": \"FirstFlrSF\",\n        \"2ndFlrSF\": \"SecondFlrSF\",\n        \"3SsnPorch\": \"Threeseasonporch\",\n    }, inplace=True,\n    )\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:05.843368Z","iopub.execute_input":"2022-08-13T20:29:05.843660Z","iopub.status.idle":"2022-08-13T20:29:05.849763Z","shell.execute_reply.started":"2022-08-13T20:29:05.843637Z","shell.execute_reply":"2022-08-13T20:29:05.848540Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Function to Load Data","metadata":{}},{"cell_type":"code","source":"def load_data():\n    # Read data\n    data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:05.876435Z","iopub.execute_input":"2022-08-13T20:29:05.877329Z","iopub.status.idle":"2022-08-13T20:29:05.883898Z","shell.execute_reply.started":"2022-08-13T20:29:05.877280Z","shell.execute_reply":"2022-08-13T20:29:05.882899Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Function to encode Data ","metadata":{}},{"cell_type":"code","source":"\n# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:05.943623Z","iopub.execute_input":"2022-08-13T20:29:05.944160Z","iopub.status.idle":"2022-08-13T20:29:05.955932Z","shell.execute_reply.started":"2022-08-13T20:29:05.944135Z","shell.execute_reply":"2022-08-13T20:29:05.954805Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Function to impute Data","metadata":{}},{"cell_type":"code","source":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:05.957664Z","iopub.execute_input":"2022-08-13T20:29:05.957959Z","iopub.status.idle":"2022-08-13T20:29:05.974400Z","shell.execute_reply.started":"2022-08-13T20:29:05.957921Z","shell.execute_reply":"2022-08-13T20:29:05.973222Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"df_train, df_test = load_data()","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:05.976105Z","iopub.execute_input":"2022-08-13T20:29:05.977279Z","iopub.status.idle":"2022-08-13T20:29:06.127207Z","shell.execute_reply.started":"2022-08-13T20:29:05.977231Z","shell.execute_reply":"2022-08-13T20:29:06.126019Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Initial Model","metadata":{}},{"cell_type":"code","source":"\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:06.129262Z","iopub.execute_input":"2022-08-13T20:29:06.129540Z","iopub.status.idle":"2022-08-13T20:29:06.136264Z","shell.execute_reply.started":"2022-08-13T20:29:06.129518Z","shell.execute_reply":"2022-08-13T20:29:06.135073Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:06.138246Z","iopub.execute_input":"2022-08-13T20:29:06.138639Z","iopub.status.idle":"2022-08-13T20:29:09.223438Z","shell.execute_reply.started":"2022-08-13T20:29:06.138604Z","shell.execute_reply":"2022-08-13T20:29:09.222649Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Mututal Information Score","metadata":{}},{"cell_type":"code","source":"\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:09.227539Z","iopub.execute_input":"2022-08-13T20:29:09.229486Z","iopub.status.idle":"2022-08-13T20:29:09.239029Z","shell.execute_reply.started":"2022-08-13T20:29:09.229447Z","shell.execute_reply":"2022-08-13T20:29:09.238365Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:09.240062Z","iopub.execute_input":"2022-08-13T20:29:09.240603Z","iopub.status.idle":"2022-08-13T20:29:10.425876Z","shell.execute_reply.started":"2022-08-13T20:29:09.240579Z","shell.execute_reply":"2022-08-13T20:29:10.424891Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Remove Uninformative Columns","metadata":{}},{"cell_type":"code","source":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.427012Z","iopub.execute_input":"2022-08-13T20:29:10.427263Z","iopub.status.idle":"2022-08-13T20:29:10.432474Z","shell.execute_reply.started":"2022-08-13T20:29:10.427241Z","shell.execute_reply":"2022-08-13T20:29:10.431280Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.433570Z","iopub.execute_input":"2022-08-13T20:29:10.433799Z","iopub.status.idle":"2022-08-13T20:29:10.444135Z","shell.execute_reply.started":"2022-08-13T20:29:10.433777Z","shell.execute_reply":"2022-08-13T20:29:10.443398Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Creating New Features","metadata":{}},{"cell_type":"code","source":"\ndef mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) / df.TotRmsAbvGrd\n\n    return X\n\n\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"Threeseasonporch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X\n\n\ndef break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.445840Z","iopub.execute_input":"2022-08-13T20:29:10.446205Z","iopub.status.idle":"2022-08-13T20:29:10.456701Z","shell.execute_reply.started":"2022-08-13T20:29:10.446173Z","shell.execute_reply":"2022-08-13T20:29:10.456028Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Clustering","metadata":{}},{"cell_type":"code","source":"\ncluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"FirstFlrSF\",\n    \"SecondFlrSF\",\n    \"GrLivArea\",\n]\n\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.458103Z","iopub.execute_input":"2022-08-13T20:29:10.458398Z","iopub.status.idle":"2022-08-13T20:29:10.476204Z","shell.execute_reply.started":"2022-08-13T20:29:10.458366Z","shell.execute_reply":"2022-08-13T20:29:10.475258Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analysis","metadata":{}},{"cell_type":"code","source":"\ndef apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.480140Z","iopub.execute_input":"2022-08-13T20:29:10.481485Z","iopub.status.idle":"2022-08-13T20:29:10.492409Z","shell.execute_reply.started":"2022-08-13T20:29:10.481422Z","shell.execute_reply":"2022-08-13T20:29:10.491062Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\ndef pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.493532Z","iopub.execute_input":"2022-08-13T20:29:10.493800Z","iopub.status.idle":"2022-08-13T20:29:10.507119Z","shell.execute_reply.started":"2022-08-13T20:29:10.493778Z","shell.execute_reply":"2022-08-13T20:29:10.506482Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Outlier Identification with PCA","metadata":{}},{"cell_type":"code","source":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.507955Z","iopub.execute_input":"2022-08-13T20:29:10.508273Z","iopub.status.idle":"2022-08-13T20:29:10.517984Z","shell.execute_reply.started":"2022-08-13T20:29:10.508246Z","shell.execute_reply":"2022-08-13T20:29:10.517295Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Target Encoding","metadata":{}},{"cell_type":"code","source":"\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.518760Z","iopub.execute_input":"2022-08-13T20:29:10.519017Z","iopub.status.idle":"2022-08-13T20:29:10.529997Z","shell.execute_reply.started":"2022-08-13T20:29:10.518986Z","shell.execute_reply":"2022-08-13T20:29:10.529387Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Final Feature Set","metadata":{}},{"cell_type":"code","source":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Lesson 2 - Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    # Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    # X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n\n    # Lesson 4 - Clustering\n    # X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n    # X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n\n    X = label_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Lesson 6 - Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nscore_dataset(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:10.530866Z","iopub.execute_input":"2022-08-13T20:29:10.531113Z","iopub.status.idle":"2022-08-13T20:29:14.609094Z","shell.execute_reply.started":"2022-08-13T20:29:10.531090Z","shell.execute_reply":"2022-08-13T20:29:14.608390Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Hyper Parameter Tuning","metadata":{}},{"cell_type":"code","source":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=6,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:14.610423Z","iopub.execute_input":"2022-08-13T20:29:14.610882Z","iopub.status.idle":"2022-08-13T20:29:36.023149Z","shell.execute_reply.started":"2022-08-13T20:29:14.610853Z","shell.execute_reply":"2022-08-13T20:29:36.022328Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Training Model","metadata":{}},{"cell_type":"code","source":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but competition loss is RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:29:36.024432Z","iopub.execute_input":"2022-08-13T20:29:36.024911Z","iopub.status.idle":"2022-08-13T20:29:41.763623Z","shell.execute_reply.started":"2022-08-13T20:29:36.024881Z","shell.execute_reply":"2022-08-13T20:29:41.762923Z"},"trusted":true},"execution_count":25,"outputs":[]}]}